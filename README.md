# ğŸ§  A Unified Deep Learning Framework for Lip Reading and Deep Fake Audio Classification

## ğŸ“Œ Project Overview
This project introduces a **deep learning-based multimodal framework** that combines **video-based lip reading** and **audio-based deepfake detection**.  
It leverages a **LipNet model** trained on the **GRID dataset** for visual speech recognition, and a **spectrogram-based CNN classifier** trained on the **Deep Voice Kaggle dataset** for detecting fake audio.  
The system is packaged into an interactive **Streamlit web app** for real-time, side-by-side video and prediction analysis.

---

## ğŸ—‚ï¸ Dataset

### ğŸ¥ Lip Reading
- **Dataset**: [GRID Corpus](https://spandh.dcs.shef.ac.uk/gridcorpus/)
- **Content**: Over 30,000 video utterances from 34 speakers
- **Pretrained Model**: [LipNet](https://github.com/rizkiarm/LipNet)
- **Preprocessing**: Mouth ROI extraction, landmark detection, frame normalization

### ğŸ”Š Deepfake Audio Detection
- **Dataset**: [Deep Voice: Deepfake Voice Recognition (Kaggle)](https://www.kaggle.com/datasets/birdy654/deep-voice-deepfake-voice-recognition)
- **Content**: Real vs. synthetic audio samples across multiple speakers
- **Preprocessing**: Mel-spectrogram extraction, normalization, padding

---

## ğŸ§  Model Architecture

### ğŸ¥ Lip Reading (Visual)
- **Backbone**: CNN â†’ Bi-GRU â†’ Dense + CTC
- **Input**: Sequence of 75 lip landmark frames
- **Output**: Decoded character tokens (CTC)

### ğŸ”Š Deepfake Detection (Audio)
- **Backbone**: 2D CNN on spectrograms
- **Input**: 128Ã—128 Mel-spectrogram images
- **Output**: Binary classification (Real or Fake)

### ğŸ” Multimodal Fusion
- Visual and audio predictions are fused using a **late decision-level fusion strategy** to enhance reliability in cross-modal authentication.

---

## âœ… Evaluation Metrics

### Lip Reading
- **Word Error Rate (WER)**: 1.74%
- **Character Error Rate (CER)**: 0.65%

### Deepfake Detection
- **Accuracy**: 98.1%
- **Precision / Recall / F1-Score**: Evaluated per class

---

## âš™ï¸ Features
- ğŸ¬ Upload and play GRID-style video samples
- ğŸ“¸ Visualize extracted lip landmarks (GIF)
- ğŸ§  Real-time lip reading using LipNet
- ğŸ”Š Audio classifier for deepfake detection
- ğŸ§ª CTC decoding and raw token visualization
- ğŸŒ Built entirely in **Streamlit** for interactivity

---

## ğŸ“Š Visual Output
- Side-by-side original video & lip-only input
- Prediction tokens and final decoded sentence
- Deepfake audio prediction (Real/Fake)
- Lip motion GIF generation for model visualization

---

## ğŸ“ Research Publication

This project is part of the **research paper** titled:  
**"A Unified Deep Learning Framework for Lip Reading and Deep Fake Audio Classification"**  
ğŸ§¾ Presented at **IEEE ISPCC 2025**  
ğŸ”— [IEEE Xplore Link](https://ieeexplore.ieee.org/document/11039390)

---

## ğŸ§  Patent

**Patent Title**: *System and Method for Real-Time Audiovisual Deepfake Detection Using Lip Reading and Audio Feature Fusion*  
- ğŸ“„ **Application No.**: 202541059722  
- ğŸ—“ï¸ **Filing Date**: June 14, 2025  
- ğŸ“… **Publication Date**: July 04, 2025  
- ğŸ¢ **Patent Office**: India  

---

## ğŸ“š Author
- **Developed by:** Omkar Bhosale

## ğŸ“« Contact

- ğŸ“§ **Email**: omkarbhosale1623@gmail.com  
- ğŸ”— **LinkedIn**: [linkedin.com/in/omkar-bhosale-75a18122a](https://www.linkedin.com/in/omkar-bhosale-75a18122a/)  
- ğŸ’» **GitHub**: [github.com/omkarbhosale1623](https://github.com/omkarbhosale1623)

